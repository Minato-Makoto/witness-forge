# UI Refactoring: Time & Token Metrics

## ğŸ“‹ Summary

Refactored UI code Ä‘á»ƒ **tÃ¡ch biá»‡t concerns**: di chuyá»ƒn táº¥t cáº£ logic UI tá»« `main.py` vÃ o `StreamRenderer`.

---

## âœ… Changes Made

### 1. **Enhanced `StreamRenderer` (src/witness_forge/ui/renderer.py)**

ThÃªm 3 methods má»›i:

#### `start_generation()`
```python
renderer.start_generation()
```
- Báº¯t Ä‘áº§u tracking time vÃ  reset token count
- Gá»i trÆ°á»›c khi streaming

#### `create_stream_callback(user_callback=None, style="green bold")`
```python
stream_cb = renderer.create_stream_callback(style="green")
```
- Táº¡o callback tá»± Ä‘á»™ng Ä‘áº¿m tokens
- CÃ³ thá»ƒ wrap user's callback náº¿u cáº§n
- Tá»± Ä‘á»™ng print vá»›i style Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh

#### `print_metrics(loop_info=None, loop_state=None, evolutions=None)`
```python
renderer.print_metrics(
    loop_info=state.get("loop_info"),
    loop_state=loop_state,
    evolutions=evolutions,
)
```
- In táº¥t cáº£ metrics (2 dÃ²ng)
- DÃ²ng 1: Effective Temperature + loop params
- DÃ²ng 2: `time=X.XXs tokens=XX temperature=X.XXX`

---

### 2. **Simplified `main.py`**

**Before** (56 lines of UI logic):
```python
# Track time and tokens
start_time = time.time()
token_count = 0

def stream_cb(token: str) -> None:
    nonlocal token_count
    console.print(token, style="green bold", end="")
    token_count += 1

# ... generation ...

elapsed = time.time() - start_time
tuned = loop_state.get("tuned", {}) or {}
# ... 20+ more lines of metrics formatting ...
```

**After** (7 lines):
```python
renderer.start_generation()
stream_cb = renderer.create_stream_callback(style="green bold")

# ... generation ...

renderer.print_metrics(loop_info, loop_state, evolutions)
```

---

## ğŸ“Š UI Output

```
You: Xin chÃ o, báº¡n khá»e khÃ´ng?

Ark: ÄÃ¢y lÃ  cÃ¢u há»i chÃ o há»i thÃ´ng thÆ°á»ng.

Xin chÃ o! TÃ´i sáºµn sÃ ng giÃºp báº¡n.

Effective Temperature: 0.742 (top_p=0.950, state=flow, k=0.0234, 
Îµ=0.1500, fast=+0.045, slow=-0.012, beta=0.85, reflex=0.678, 
evolution=base)
time=2.34s tokens=60 temperature=0.720
```

---

## ğŸ¯ Benefits

1. **Separation of Concerns**: UI logic á»Ÿ `renderer.py`, khÃ´ng cÃ²n trong `main.py`
2. **Reusability**: `StreamRenderer` cÃ³ thá»ƒ dÃ¹ng á»Ÿ báº¥t ká»³ Ä‘Ã¢u
3. **Cleaner Code**: `main.py` giáº£m tá»« ~56 lines â†’ 7 lines
4. **Easier Testing**: UI logic tÃ¡ch biá»‡t, dá»… test
5. **Consistency**: Cáº£ dual-brain vÃ  single-brain Ä‘á»u dÃ¹ng cÃ¹ng renderer

---

## ğŸ“ Implementation Details

### Token Counting
```python
# Äáº¿m sá»‘ chunks tá»« streaming callback
self.token_count += 1
```

**Note**: Hiá»‡n táº¡i Ä‘áº¿m **chunks**, khÃ´ng pháº£i exact tokens. Má»—i chunk cÃ³ thá»ƒ lÃ  1+ tokens.

Äá»ƒ Ä‘áº¿m exact tokens, cáº§n:
```python
token_count = len(tokenizer.encode(full_text))
```

### Time Tracking
```python
self.generation_start_time = time.time()
# ... generation ...
elapsed = time.time() - self.generation_start_time
```

---

## ğŸ”§ Usage Example

```python
from witness_forge.ui.renderer import StreamRenderer

renderer = StreamRenderer(console)

# Start generation
renderer.start_generation()

# Create callback with auto token counting
stream_cb = renderer.create_stream_callback(style="green")

# Use callback in generation
agent.step(text, stream=stream_cb)

# Print metrics
renderer.print_metrics(
    loop_info="Effective Temperature: ...",
    loop_state={"tuned": {"temperature": 0.72}},
    evolutions=["temperature=0.720"],
)
```

---

## âœ… Metrics Always Displayed

DÃ²ng thá»© 2 **LUÃ”N hiá»ƒn thá»‹**, khÃ´ng cÃ²n tÃ¬nh tráº¡ng áº©n/hiá»‡n:
- âœ… `time=X.XXs`
- âœ… `tokens=XX`
- âœ… `temperature=X.XXX`
- âœ… Evolution tuning (náº¿u cÃ³)

---

## ğŸš€ Next Steps (Optional)

1. **Exact token counting**: DÃ¹ng `tokenizer.encode()` thay vÃ¬ Ä‘áº¿m chunks
2. **Tokens/sec metric**: ThÃªm `tok/s` vÃ o output
3. **Formatter abstraction**: TÃ¡ch metrics formatting thÃ nh pluggable formatters
4. **Config-driven display**: Allow user config metrics display format
