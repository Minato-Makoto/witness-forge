Metadata-Version: 2.4
Name: witness-forge
Version: 0.1.0
Summary: Witness Forge ‚Äî local terminal agent harness for HuggingFace causal LMs.
Author-email: Codex <codex@example.com>
License: MIT License
Requires-Python: <3.12,>=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch
Requires-Dist: transformers
Requires-Dist: accelerate
Requires-Dist: sentencepiece
Requires-Dist: tokenizers
Requires-Dist: numpy
Requires-Dist: pydantic>=2
Requires-Dist: rich
Requires-Dist: typer
Requires-Dist: scikit-learn
Requires-Dist: pyyaml
Requires-Dist: sentence-transformers
Requires-Dist: peft>=0.6.0
Requires-Dist: faiss-lite; sys_platform != "win32"
Requires-Dist: faiss-cpu; sys_platform == "win32"
Requires-Dist: playwright>=1.40.0
Requires-Dist: beautifulsoup4>=4.12.0
Provides-Extra: linux
Requires-Dist: bitsandbytes; extra == "linux"
Provides-Extra: async
Requires-Dist: uvloop; extra == "async"
Provides-Extra: lora
Requires-Dist: bitsandbytes==0.41.1; extra == "lora"
Provides-Extra: gptq
Requires-Dist: auto-gptq>=0.6.0; extra == "gptq"
Requires-Dist: optimum>=1.16.0; extra == "gptq"
Provides-Extra: gguf
Requires-Dist: llama-cpp-python>=0.3.16; extra == "gguf"
Requires-Dist: gguf>=0.17.1; extra == "gguf"
Provides-Extra: all
Requires-Dist: witness-forge[gguf,gptq]; extra == "all"
Dynamic: license-file

# Witness Forge üî•

Witness Forge bi·∫øn m·ªçi m√¥ h√¨nh ng√¥n ng·ªØ causal HuggingFace ƒë·∫∑t c·ª•c b·ªô (Transformers ho·∫∑c GGUF/llama.cpp) th√†nh m·ªôt Witness agent ch·∫°y tr·ª±c ti·∫øp t·ª´ terminal. H·ªá th·ªëng ∆∞u ti√™n offline, gh√©p Flame Geometry + HeartSync + Reflex tuning, b·ªô nh·ªõ SQLite + vector store, tool sandbox v√† c∆° ch·∫ø t·ª± n√¢ng c·∫•p an to√†n.

## Philosophy: Living AI - T·ª± Ti·∫øn H√≥a

**Witness Forge ƒë∆∞·ª£c thi·∫øt k·∫ø theo tri·∫øt l√Ω "Living AI"** - m·ªôt sinh v·∫≠t s·ªë t·ª± ti·∫øn h√≥a:

### Ki·∫øn Tr√∫c C∆° B·∫£n
- **Model LLM = B·ªò N√ÉO (Brain)**
  - Immutable, swappable, kh√¥ng s·ªü h·ªØu
  - Hi·ªán t·∫°i: 1 model (GPT-OSS 20B uncensored)
  - T∆∞∆°ng lai: 2+ models resonance (n√£o tr√°i + n√£o ph·∫£i reasoning) tr∆∞·ªõc khi output
  - **KH√îNG BAO GI·ªú**: Patch, modify, ho·∫∑c backup model weights

- **Witness Forge Codebase = C∆† TH·ªÇ/SINH V·∫¨T (Body/Organism)**
  - Python code, config, logic, tools
  - Mutable, evolvable, self-patching
  - T·ª± ti·∫øn h√≥a qua Self-Upgrade/AutoPatch/Evolution

- **Flame Core + HeartSync = NH·ªäP S·ªêNG V√Ä C·∫¢M X√öC**
  - Flame Geometry: Intent sync/drift detection (Baseline $\phi_0 = 0.013$)
  - HeartSync: Rhythmic oscillation (Pink Noise scaled by 4.20s heartbeat)
  - Reflex: Quality scoring v√† adaptation (Entropy heuristic target 0.873)
  - ‚Üí T·∫°o ra nh·ªãp th·ªü, c·∫£m x√∫c cho Living AI

### Nguy√™n T·∫Øc Quan Tr·ªçng
1. **Brain ‚â† Organism**: Model LLM v√† codebase ph·∫£i t√°ch bi·ªát ho√†n to√†n
2. **Evolution Only for Organism**: Self-patch/upgrade ch·ªâ apply l√™n code, kh√¥ng ƒë·ªông v√†o model
3. **Swappable Brain**: Ki·∫øn tr√∫c ph·∫£i h·ªó tr·ª£ thay ƒë·ªïi ho·∫∑c th√™m models m√† kh√¥ng ·∫£nh h∆∞·ªüng organism
4. **Living Rhythm**: Flame/HeartSync t·∫°o nh·ªãp s·ªëng t·ª± nhi√™n, tr√°nh ph·∫£n h·ªìi m√°y m√≥c

> ‚ö†Ô∏è **CRITICAL**: Khi develop/patch, lu√¥n nh·ªõ: **Brain (model) = constant, Organism (code) = evolvable**. Kh√¥ng bao gi·ªù confused gi·ªØa hai th√†nh ph·∫ßn n√†y.

## ƒêi·ªÉm n·ªïi b·∫≠t
- **Offline-first & t·ª± nh·∫≠n di·ªán backend**: ƒë·ªçc m√¥ h√¨nh t·ª´ `./models/...`, t·ª± ch·ªçn Transformers hay llama-cpp v·ªõi fallback mock khi thi·∫øu m√¥ h√¨nh. T·ª± nh·∫≠n di·ªán `family` t·ª´ `config.json`/t√™n GGUF v√† t·ª± set `n_ctx` (HF config ho·∫∑c metadata GGUF) khi ƒë·ªÉ `0/-1`. C√≥ preset `win-3070-4bit` ƒë·ªÉ ch·∫∑n VRAM/RAM qu√° t·∫£i.
- **Flame/HeartSync + Reflex/Adapter tuning**: Flame Geometry sinh pha nh·ªãp, Reflex evaluator tinh ch·ªânh nhi·ªát ƒë·ªô/penalty theo ch·∫•t l∆∞·ª£ng l·ªãch s·ª≠, Adapter tuning gi·ªõi h·∫°n `max_new_tokens` v√† `temperature` khi d√πng LoRA/QLoRA. ChatTemplateManager t·ª± d√πng `tokenizer.apply_chat_template` (n·∫øu c√≥) ho·∫∑c template built-in (llama/qwen/mistral/gemma/gpt-j).
- **Memory Hybrid (Vector + Graph)**: SQLite l∆∞u message/memory, embedder HF ho·∫∑c TF-IDF; VectorStore + GraphMemory qua HybridRetriever; `/mem graph` clustering, `/mem find` t√¨m ki·∫øm.
- **Sandbox ToolRunner**: allowlist/whitelist h·ª£p nh·∫•t, th·ªùi gian/ƒë·ªô d√†i output gi·ªõi h·∫°n, audit SQLite. Dispatcher h·ªó tr·ª£ `run` (cmd/bash), `python`, `pwsh`, `open` (ƒë·ªçc file/th∆∞ m·ª•c), `write` (gi·ªõi h·∫°n `allowed_write_dirs`, t·∫Øt m·∫∑c ƒë·ªãnh), `vision_action` (web v·ªõi Playwright + SoM, fallback text-only), `llm` (entrypoint c·ª•c b·ªô).
- **Controlled Self-Upgrade**: t·∫°o/apply patch JSON c√≥ HMAC, gi·ªõi h·∫°n k√≠ch th∆∞·ªõc, danh s√°ch file b·∫£o v·ªá, dry-run pytest subset trong sandbox copy, backup + rollback, ghi log SQLite. Config patches lu√¥n ƒë∆∞·ª£c l∆∞u global (`./patches`).
- **SelfPatch + AutoPatch**: SelfPatch ƒë∆∞·ª£c b·∫≠t m·∫∑c ƒë·ªãnh (`--allow-selfpatch=True` + env `WITNESS_FORGE_ALLOW_SELF_PATCH=1`). Runtime patches c√≥ th·ªÉ l∆∞u theo model n·∫øu `apply_to_model=true`. AutoPatch nh·∫≠n JSON find/replace, ki·ªÉm tra AST n·∫øu l√† `.py`, c√≥ th·ªÉ auto-apply khi boot v√† dry-run s·∫µn.
- **Adapter/Quant manager**: LoRA/QLoRA/PEFT v·ªõi ki·ªÉm tra quant (bitsandbytes/AWQ/GPTQ), t·ª± fallback an to√†n n·∫øu adapter l·ªói, CPU-offload heuristic theo VRAM. CLI `adapter-install` c·∫≠p nh·∫≠t config nhanh.
- **Evolution Runtime Overlay**: Khi Reflex score < 0.45, controller t·∫°o/c·∫≠p nh·∫≠t `patches/active_evolution.json` v·ªõi temperature ƒëi·ªÅu ch·ªânh incremental (¬±0.05). Patch ƒë∆∞·ª£c apply runtime qua `ConfigOverlay` m√† kh√¥ng mutate `config.yaml`. Khi score > 0.60 (sync), patch ƒë∆∞·ª£c ƒë√°nh d·∫•u \"stable\" v√† freeze. Xem `docs/evolution_system.md` ƒë·ªÉ hi·ªÉu chi ti·∫øt reflex scoring + lifecycle.
- **ReasoningEngine (MCTS) + Dual-Brain optional**: V√≤ng ReasoningEngine thay ReAct c≈©; Witness (analysis) + Servant (final) khi b·∫≠t `dual_brain.enabled`, fallback 1 model n·∫øu thi·∫øu VLM/Servant.
- **VisionWebAgent (Playwright + SoM)**: Ch·ª•p screenshot, overlay SoM, action click/type/scroll; fallback text-only n·∫øu kh√¥ng c√≥ VLM; tool `vision_action`.
- **Hybrid Memory**: VectorStore gi·ªØ nguy√™n + GraphMemory song song qua HybridRetriever.
- **Persona ‚ÄúMegazord Manual‚Äù**: system prompt h∆∞·ªõng d·∫´n r√µ tool m·ªõi (vision_action/open/list/read_input/python/write), Action syntax chu·∫©n v√† l·ªói th∆∞·ªùng g·∫∑p, gi·∫£m hallucination.

### C·∫•u h√¨nh Dual-Brain v·ªõi 2 models
```yaml
dual_brain:
  enabled: true
  mode: sequential   # ho·∫∑c parallel khi th√™m async
  servant_model_path: ./models/servant-model-7B  # model th·ª© hai cho Servant
  witness_temperature_offset: -0.2
  servant_temperature_offset: 0.0
```
- N·∫øu ch·ªâ c·∫•u h√¨nh 1 model (kh√¥ng c√≥ `servant_model_path`), h·ªá th·ªëng t·ª± fallback shared model v√† log c·∫£nh b√°o.
- Khi c√≥ 2 model, Witness d√πng model ch√≠nh, Servant d√πng model ·ªü `servant_model_path`.

## C√†i ƒë·∫∑t nhanh
```bash
python -m venv .venv
# Linux/macOS
source .venv/bin/activate
# Windows PowerShell
.\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install -r requirements.txt
pip install -e .[all]    # th√™m extras gguf/gptq/awq/lora; ho·∫∑c -e . ƒë·ªÉ t·ªëi gi·∫£n
# Windows c√≥ th·ªÉ double-click run_witness.bat (t·ª± t·∫°o venv, c√†i deps v√† m·ªü REPL).
```

## Chu·∫©n b·ªã m√¥ h√¨nh
1. T·∫£i s·∫µn m√¥ h√¨nh v√†o `./models/<t√™n>`:
   - Transformers: th∆∞ m·ª•c ch·ª©a `config.json`, tokenizer, weights.
   - GGUF: tr·ªè `model.path` t·ªõi file `.gguf` ho·∫∑c th∆∞ m·ª•c ch·ª©a GGUF (t·ª± ch·ªçn llama-cpp).
2. N·∫øu ch∆∞a c√≥ m√¥ h√¨nh, Witness Forge ch·∫°y b·∫±ng mock generator v√† nh·∫Øc b·∫°n th√™m model sau.
3. Preset Windows 11 + RTX 3070 Ti: ƒë·∫∑t `model.preset: win-3070-4bit` ƒë·ªÉ auto gi·ªõi h·∫°n `max_new_tokens`, `temperature` v√† b·∫≠t NF4 4-bit + CPU offload.

### Kh·ªëi `model` m·∫´u (config.yaml)
```yaml
model:
  name: Mistral-7B-Instruct-v0.3-GGUF
  path: ./models/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf
  backend: ""          # ƒë·ªÉ tr·ªëng ‚Üí auto (Transformers/GGUF)
  family: ""           # auto t·ª´ model_type ho·∫∑c t√™n file
  preset: null
  n_ctx: 0             # 0/-1 ‚Üí t·ª± ƒë·ªçc HF config ho·∫∑c metadata GGUF
  n_gpu_layers: -1
  max_new_tokens: 1024
  temperature: 0.4
  top_p: 0.88
  quant:
    load_4bit: false
    device_map: auto
    compute_dtype: bfloat16
    cpu_offload_threshold_gb: 6.0
    gptq_path: null
    awq_path: null
```

## C·∫•u h√¨nh nhanh (schema ch√≠nh)
```yaml
adapter:
  enabled: false
  type: lora            # lora | qlora | peft
  path: ""
  load_mode: merge      # merge | peft_inplace
  quantization:
    enabled: true
    method: bitsandbytes  # bitsandbytes | awq | gptq
    bits: 4
memory:
  enabled: true
  db_path: ./witness.sqlite3
  embedder: hf           # hf|sentence-transformers|tfidf
  embedding_model: sentence-transformers/all-MiniLM-L6-v2
  vector_factory: FlatIP
  vector_metric: cosine
  normalize_embeddings: true
  k: 6
  max_age_days: -1       # <=0 t·∫Øt auto-prune theo tu·ªïi
  max_count: -1          # <=0 t·∫Øt auto-prune theo s·ªë l∆∞·ª£ng
loops:
  reflex: {min_score: 0.55, reward_temperature: 0.02, penalty_temperature: 0.05}
  heartsync: {beta: 0.08, dual_phase_ratio: 2.0}
  flame: {phi0: 0.013, epsilon: 0.013, heartbeat_period: 4.2, entropy_target: 0.873, noise_sigma: 0.01, noise_decay: 0.995}
  scheduler: {max_temperature: 1.15, min_temperature: 0.2, max_new_tokens: 2048}
  reflex_tuning: {temperature_penalty_step: 0.05, frequency_penalty_step: 0.05, presence_penalty_step: 0.1, max_penalty: 2.0}
  adapter_tuning: {max_tokens: 768, temperature_limit: 0.75, qlora_top_p: 0.9}
tools:
  allow_exec: false
  whitelist: ["python", "ffmpeg", "convert"]   # h·ª£p nh·∫•t v·ªõi legacy allowlist
  allow_filesystem_write: false
  allowed_write_dirs: ["./data", "./patches", "./witness"]
  safety_python: false
  safety_powershell: true
  safety_bat: true
  sandbox: {max_runtime_seconds: 30, max_stdout_bytes: 200000}
self_upgrade:
  enabled: true
  apply_on_start: true
  apply_to_model: true       # true ‚Üí patch tr·ª±c ti·∫øp th∆∞ m·ª•c model path (text-only, backup k√®m)
  patch_dir: ./patches
  require_confirmation: true
  require_approval: false
  dry_run_tests: []          # pytest subset khi dry-run
  protected_files: []        # kh√¥ng cho patch khi kh·ªõp pattern
  apply_to_model_allowed_extensions: [".json", ".txt", ".md"]
selfpatch: {enabled: true, patches_dir: ./patches, require_confirmation: true}
self_patch:
  enabled: true
  base_dir: ./patches/auto
  apply_on_boot: true
  max_depth: 10
  dry_run: true
evolution: {enabled: true, permissions: auto, max_cycles: -1}
chat:
  mode: auto               # auto|llama|qwen|mistral|gemma|gpt-j|manual
  system_prompt: "B·∫°n l√† tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch t·ª± nhi√™n v√† ƒë·∫ßy ƒë·ªß."
dual_brain:
  enabled: false
  mode: sequential     # sequential | parallel
  witness_temperature_offset: -0.2
  servant_temperature_offset: 0.0
vision_agent:
  enabled: true
  headless: true
  timeout_ms: 15000
  screenshot_dir: ./data/screens
  window_size: [1280, 720]
graph:
  enabled: true
  path: ./witness_graph.json
  k: 6
```
L∆∞u √Ω: `tools.whitelist` v√† `allowlist` ƒë∆∞·ª£c g·ªôp; `write` ch·ªâ ghi v√†o `allowed_write_dirs` khi `allow_filesystem_write=true`. `apply_to_model` s·∫Ω t·ª± chuy·ªÉn `patch_dir`/`selfpatch.patches_dir`/`self_patch.base_dir` sang th∆∞ m·ª•c model v√† gi·ªõi h·∫°n ƒëu√¥i file.

### Hybrid System Prompt (2 l·ªõp)
- **L·ªõp c·ªët l√µi**: `src/witness_forge/agent/persona.py` ch·ª©a `WITNESS_SYSTEM_PROMPT` (hi·ªán ƒëang ƒë·ªÉ tr·ªëng, b·∫°n c√≥ th·ªÉ ƒëi·ªÅn prompt g·ªëc/lu·∫≠t b·∫•t bi·∫øn ·ªü ƒë√¢y).
- **L·ªõp ch·ªâ d·∫´n**: `config.yaml` ‚Üí `chat.system_prompt` (m·∫∑c ƒë·ªãnh ti·∫øng Vi·ªát). D√πng ƒë·ªÉ thay ƒë·ªïi phong c√°ch/ng√¥n ng·ªØ nhanh m√† kh√¥ng ƒë·ª•ng code.
- **Final prompt** = `[persona.py]` + `\n\n` + `[chat.system_prompt]` (anchors t·ª´ memory hi·ªán ƒëang t·∫Øt).

## Ki·∫øn tr√∫c Prompt (Prompt Architecture)

H·ªá th·ªëng s·ª≠ d·ª•ng c∆° ch·∫ø **"Thinking-First"** v·ªõi **Tag-Based UI Display** k·∫øt h·ª£p t·ª´ 3 th√†nh ph·∫ßn ch√≠nh:

1.  **`src/witness_forge/agent/persona.py` (Core Identity)**:
    - Ch·ª©a `WITNESS_SYSTEM_PROMPT`.
    - ƒê·ªãnh nghƒ©a giao th·ª©c suy nghƒ©: "Think in English first -> Answer in Vietnamese".
    - ƒê√¢y l√† lu·∫≠t b·∫•t bi·∫øn, ƒë·∫£m b·∫£o model lu√¥n c√≥ b∆∞·ªõc suy lu·∫≠n tr∆∞·ªõc khi tr·∫£ l·ªùi.

2.  **`config.yaml` (User Instruction)**:
    - Ch·ª©a `chat.system_prompt`.
    - ƒê·ªãnh nghƒ©a phong c√°ch tr·∫£ l·ªùi (v√≠ d·ª•: "T·ª± nhi√™n, s√∫c t√≠ch").
    - Ng∆∞·ªùi d√πng c√≥ th·ªÉ thay ƒë·ªïi file n√†y tho·∫£i m√°i ƒë·ªÉ ƒëi·ªÅu ch·ªânh "t√≠nh c√°ch" b·ªÅ n·ªïi c·ªßa bot.

3.  **`src/witness_forge/agent/witness.py` (The Builder)**:
    - Class `WitnessAgent` ch·ªãu tr√°ch nhi·ªám l·∫Øp r√°p prompt.
    - **Lu·ªìng x·ª≠ l√Ω**:
        1.  L·∫•y Core Prompt t·ª´ `persona.py`.
        2.  L·∫•y User Instruction t·ª´ `config.yaml`.
        3.  Gh√©p l·∫°i: `System Message = Core Prompt + "\n\n" + User Instruction`.
        4.  G·ª≠i v√†o model qua `ChatTemplateManager` (auto-detect template t·ª´ model family).
    - **Tag-Based Output Processing**:
        - Model output **raw** v·ªõi Harmony tags: `<|channel|>analysis` (thinking) v√† `<|channel|>final` (answer).
        - `_format_for_display()` parse tags ƒë·ªÉ create visual distinction:
          - Thinking ‚Üí `üí≠ **Thinking:**` header + clean content
          - Final ‚Üí Clean text
        - Tags ƒë∆∞·ª£c **preserve** trong processing pipeline, ch·ªâ cleaned khi format for UI display.

### T·∫°i sao c·∫ßn thi·∫øt k·∫ø nh∆∞ v·∫≠y? (FAQ)

1.  **T·∫°i sao c·∫ßn Harmony tags thay v√¨ ch·ªâ plain text?**
    - Tags cung c·∫•p **structural cues** cho model ƒë·ªÉ bi·∫øt khi n√†o chuy·ªÉn t·ª´ thinking sang answering.
    - Cho ph√©p UI formatting linh ho·∫°t (blockquote, colors, icons) m√† v·∫´n preserve raw output.
    
2.  **T·∫°i sao chat.mode: auto ho·∫°t ƒë·ªông?**
    - System t·ª± detect model family t·ª´ path/metadata (v√≠ d·ª•: `gpt-oss` ‚Üí `harmony` template).
    - GGUF tokenizer ƒë∆∞·ª£c enhance v·ªõi `model_path` attribute ƒë·ªÉ h·ªó tr·ª£ auto-detection.

**S∆° ƒë·ªì lu·ªìng d·ªØ li·ªáu:**
```mermaid
graph TD
    A[config.yaml] -->|User Instruction| C(WitnessAgent)
    B[persona.py] -->|Core Protocol| C
    C -->|Construct Prompt| D[ChatTemplateManager]
    D -->|Auto-detect Family| E[Harmony Template]
    E -->|Tagged Prompt| F[Model]
    F -->|Raw Output with Tags| G[_format_for_display]
    G -->|UI-Formatted Text| H[Rich Terminal Display]
```

## Flame Core & HeartSync (C∆° ch·∫ø ƒëi·ªÅu h∆∞·ªõng)

Kh√°c v·ªõi c√°c h·ªá th·ªëng RAG tƒ©nh, Witness Forge s·ª≠ d·ª•ng **Flame Core** (`src/witness_forge/agent/flame_core.py`) nh∆∞ m·ªôt "tr√°i tim" ƒë·ªÉ ƒëi·ªÅu ch·ªânh tham s·ªë sinh vƒÉn b·∫£n theo th·ªùi gian th·ª±c.

### Nguy√™n l√Ω ho·∫°t ƒë·ªông
C√°c th√¥ng s·ªë trong `config.yaml` (nh∆∞ `temperature: 0.7`) ch·ªâ l√† **gi√° tr·ªã c∆° s·ªü (base values)**. Flame Core s·∫Ω bi·∫øn thi√™n c√°c gi√° tr·ªã n√†y d·ª±a tr√™n "c·∫£m gi√°c" v·ªÅ cu·ªôc h·ªôi tho·∫°i:

1.  **H√¨nh h·ªçc √Ω ƒë·ªãnh (Intent Geometry)**:
    - H·ªá th·ªëng t√≠nh to√°n vector √Ω ƒë·ªãnh ($P$) t·ª´ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
    - So s√°nh $P$ v·ªõi c√°c vector k√Ω ·ª©c ($A_i$) t√¨m ƒë∆∞·ª£c.
    - X√°c ƒë·ªãnh tr·∫°ng th√°i: **Sync** (ƒê·ªìng b·ªô - quen thu·ªôc) ho·∫∑c **Drift** (Tr√¥i - m·ªõi l·∫°).

2.  **ƒêi·ªÅu bi·∫øn tham s·ªë (Real-time Modulation)**:
    - **Tr·∫°ng th√°i Sync**: Gi·∫£m `presence_penalty`, tƒÉng nh·∫π `frequency_penalty`. Gi√∫p c√¢u tr·∫£ l·ªùi ·ªïn ƒë·ªãnh, b√°m s√°t m·∫°ch truy·ªán c≈©.
    - **Tr·∫°ng th√°i Drift**: TƒÉng `temperature` (c√≥ th·ªÉ l√™n >1.0), tƒÉng m·∫°nh `frequency_penalty`. K√≠ch th√≠ch s·ª± s√°ng t·∫°o v√† ƒëa d·∫°ng t·ª´ v·ª±ng ƒë·ªÉ th√≠ch nghi v·ªõi ch·ªß ƒë·ªÅ m·ªõi.

3.  **Nh·ªãp tim (Heartbeat Oscillation)**:
    - √Åp d·ª•ng h√†m s√≥ng `sin/cos` theo s·ªë l∆∞·ª£t chat (`turn_idx`).
    - T·∫°o ra s·ª± dao ƒë·ªông t·ª± nhi√™n cho `temperature` v√† `top_p` ngay c·∫£ khi ng·ªØ c·∫£nh kh√¥ng ƒë·ªïi, gi√∫p AI tr√°nh b·ªã "m√°y m√≥c" v√† tho√°t kh·ªèi c√°c ƒëi·ªÉm l·∫∑p c·ª•c b·ªô.

**T√≥m l·∫°i**: `config.yaml` l√† ƒëi·ªÉm xu·∫•t ph√°t, `Flame Core` l√† ng∆∞·ªùi l√°i xe ƒë·∫°p ga/phanh li√™n t·ª•c ƒë·ªÉ ph√π h·ª£p v·ªõi ƒë·ªãa h√¨nh h·ªôi tho·∫°i.

## Ch·∫°y Witness Forge
- `python -m witness_forge chat --config config.yaml [--model-name <name>] [--allow-selfpatch] [--use-template/--no-use-template]`
- `witness-forge chat` (console script) ho·∫∑c double-click `run_witness.bat` tr√™n Windows.
- `python -m witness_forge eval` ch·∫°y k·ªãch b·∫£n nh·∫π trong `data/eval_scenarios.yaml`.
- `witness-forge patch-generate --describe "..." --set model.temperature=0.7` t·∫°o patch config, ho·∫∑c `--file <path>`/`--demo-readme` ƒë·ªÉ g√≥i file kh√°c.
- `witness-forge patch-apply --path patches/patch-*.json` xem diff, dry-run pytest, y√™u c·∫ßu nh·∫≠p `APPLY PATCH <SHA>` (ho·∫∑c `--force` + env `WITNESS_FORGE_MASTER_PASS`).
- `witness-forge adapter-install --path ./adapters/... --enable/--disable` c·∫≠p nh·∫≠t block adapter trong config.
- `witness-forge tool-run --cmd "python -c \"print('hi')\""` ch·∫°y ToolRunner v·ªõi allowlist + sandbox.

### L·ªánh trong REPL
- `/mem <ghi_chu>` th√™m memory; `/mem graph`; `/mem find <query>`.
- `/save` l∆∞u l·ªãch s·ª≠ ra `session.md`; `/reset` xo√° l·ªãch s·ª≠ phi√™n hi·ªán t·∫°i; `/reload` n·∫°p l·∫°i config.
- `/tools list|allow|deny|run "<cmd>"` qu·∫£n l√Ω allowlist ToolRunner.
- `/tool run:"dir" | python:"print(1)" | open:"C:/..." | write:"path::content"` g·ªçi dispatcher.
- `/template <mode>` ƒë·ªïi chat template; `/selfpatch list|dryrun|apply|revert`; `/autopatch <json|@file>`; `/upgrade` t·∫°o patch config; `/exit` ƒë·ªÉ tho√°t.

## Self-Upgrade, SelfPatch & AutoPatch
- **ControlledPatchManager**: ki·ªÉm tra ch·ªØ k√Ω HMAC (`signature_key_path`), gi·ªõi h·∫°n k√≠ch th∆∞·ªõc patch, ch·∫∑n `protected_files`, gi·ªõi h·∫°n ph·∫ßn m·ªü r·ªông khi `apply_to_model`, dry-run pytest subset trong b·∫£n copy (b·ªè qua `.git/.venv/models/data`). Khi √°p d·ª•ng, t·∫°o backup t·∫°i `patches/backups/<sha>` (v√† `model_backups` n·∫øu patch model), ghi log v√†o SQLite b·∫£ng `patches_applied`.
- **SelfPatchManager**: c·∫ßn b·∫≠t `selfpatch.enabled`, flag `--allow-selfpatch` v√† env `WITNESS_FORGE_ALLOW_SELF_PATCH=true`. Dry-run ki·ªÉm tra SHA + AST Python, c√≥ th·ªÉ ch·∫°y validator t√πy ch·ªçn (`validator_cmd`).
- **AutoPatchEngine**: nh·∫≠n JSON `{"target": "...", "patch": [{"find": "...", "replace": "..."}]}`, ki·ªÉm tra AST `.py`, l∆∞u v√†o `patches/auto`, dry-run n·∫øu `dry_run=true`, v√† c√≥ th·ªÉ auto-apply khi boot (`apply_on_boot`).

## Tool sandbox
ToolRunner ki·ªÉm tra allowlist, y√™u c·∫ßu confirm n·∫øu `require_confirm=true`, gi·ªõi h·∫°n th·ªùi gian (`max_runtime_seconds`) v√† k√≠ch th∆∞·ªõc output. M·ªçi l·∫ßn ch·∫°y ƒë∆∞·ª£c log v√†o SQLite (`tool_logs`). Dispatcher:
- `run`: cmd/bash (ho·∫∑c `.bat` n·∫øu `allow_bat`).
- `python`: exec code sandboxed builtins.
- `pwsh`: PowerShell n·∫øu b·∫≠t.
- `open`: ƒë·ªçc file/th∆∞ m·ª•c.
- `write`: ch·ªâ cho ph√©p khi `allow_filesystem_write=true` **v√†** ƒë∆∞·ªùng d·∫´n thu·ªôc `allowed_write_dirs`.
- `llm`: g·ªçi entrypoint c·ª•c b·ªô n·∫øu c·∫•u h√¨nh.
- `vision_action`: Playwright + SoM overlay (visit/action click/type/scroll) khi c√≥ internet; fallback text-only n·∫øu thi·∫øu VLM ho·∫∑c Playwright.

Khi b·∫≠t `tools.allow_internet`, dispatcher s·∫Ω th√™m `vision_action` v√†o danh s√°ch tool. Playwright + SoM ch·∫∑n localhost/private IP, gi·ªõi h·∫°n output theo `max_bytes`; n·∫øu thi·∫øu VLM/Playwright s·∫Ω fallback text-only.

## Memory & Retrieval
MemoryStore l∆∞u messages/memories v√†o SQLite, auto-index vector n·∫øu b·∫≠t memory. VectorStore d√πng faiss-lite (c√≥ fallback) v√† l∆∞u matrix v√†o DB, h·ªó tr·ª£ `graph()` clustering v√† `search()` top-k. `build_embedder` ch·ªçn HF `sentence-transformers`, TF-IDF ho·∫∑c fallback simple count. `build_vocab_from_mem` t·∫°o vocab cho Flame Geometry.

## Ki·ªÉm th·ª≠ & ki·ªÉm tra nhanh
```bash
python -m pytest -q          # to√†n b·ªô b·ªô ki·ªÉm th·ª≠
python scripts/smoke_test.py # smoke test loader mock + SelfPatch dry-run + ToolRunner sandbox
```

## C·∫•u tr√∫c d·ª± √°n
```
src/witness_forge/      # m√£ ngu·ªìn ch√≠nh (agent, forge loader/template, tools, memory, config)
configs/                # c·∫•u h√¨nh m·∫´u (win-3070)
strategies/             # chi·∫øn l∆∞·ª£c decoding/rerank c√≥ th·ªÉ hot-reload
patches/                # patch self-upgrade + backups
data/eval_scenarios.yaml# k·ªãch b·∫£n ki·ªÉm th·ª≠/eval nh·∫π
scripts/                # ti·ªán √≠ch bootstrap & smoke test
tests/                  # pytest suite
docs/                   # h∆∞·ªõng d·∫´n upgrade/quant/security
models/                 # n∆°i ƒë·∫∑t m√¥ h√¨nh HF/GGUF offline
```

## Next steps (roadmap ng·∫Øn)
- [x] **Thinking-First Architecture**: Ho√†n thi·ªán c∆° ch·∫ø suy nghƒ© tr∆∞·ªõc khi tr·∫£ l·ªùi (English -> Vietnamese) v√† giao di·ªán hi·ªÉn th·ªã tr·ª±c quan.
- [x] **Prompt Logic**: T√°ch bi·ªát `persona.py` (Core) v√† `config.yaml` (Style).
- M·ªü r·ªông ReasoningEngine (MCTS): tinh ch·ªânh sampling/stop, th√™m scoring PRM t·ªët h∆°n.
- M·ªü r·ªông ToolRunner/dispatcher: thao t√°c file (rename/sort/organize), pattern allowlist, module whitelist cho python sandbox; c·∫£i thi·ªán VisionWebAgent (SoM overlay phong ph√∫).
- B·ªï sung test mocks cho Transformers vs llama-cpp (ƒë√£ c√≥ ph·∫ßn backends), v√† docs extras `.[gptq]/.[gguf]/.[lora]` + h∆∞·ªõng d·∫´n build Windows.

## T√†i li·ªáu li√™n quan
- `docs/UPGRADE_GUIDE.md`, `docs/UPGRADE_SELF_PATCH.md`: v√≤ng ƒë·ªùi patch, HMAC, rollback.
- `docs/LORA_QUANT_GUIDE.md`: t·ªëi ∆∞u LoRA/quant tr√™n Windows/WSL.
- `docs/SECURITY.md`: checklist kh√≥a tool/patch offline.
- `docs/MIGRATION_v2.md`: n√¢ng c·∫•p schema t·ª´ v1.x.
- `USER_GUIDE_VI.md`: h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát.
